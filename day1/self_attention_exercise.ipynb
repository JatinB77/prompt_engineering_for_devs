{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364e819c",
   "metadata": {},
   "source": [
    "# Transformers and self-attention exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ae9c9",
   "metadata": {},
   "source": [
    "### Self-Attention Mechanism\n",
    "\n",
    "Self-attention allows the model to weigh the importance of different words in a sentence relative to each other.\n",
    "It helps capture relationships and dependencies between words regardless of their positions in the sentence.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of calculating attention scores once, multi-head attention splits the computations into multiple parallel \"heads\".\n",
    "Each head performs its own self-attention calculations on a different linear projection of the input.\n",
    "This allows the model to focus on different parts of the sentence simultaneously, capturing various aspects of the relationships between words.\n",
    "Number of Heads (num_heads):\n",
    "\n",
    "### The number of attention heads in the multi-head attention mechanism\n",
    "\n",
    "If num_heads is 12, for example, the model will have 12 separate sets of attention calculations running in parallel.\n",
    "Each head operates on a different linear transformation of the input, allowing the model to capture diverse patterns.\n",
    "\n",
    "\n",
    "### Code explanation \n",
    "\n",
    "* The shape of the attention tensor is [batch_size, num_heads, sequence_length, sequence_length].\n",
    "* For a single sentence (batch_size = 1), the shape is [1, num_heads, sequence_length, sequence_length].\n",
    "* In the visualization, attention[0] refers to the first head (index 0) of the multi-head attention mechanism. That is, the heatmap for attention[0] shows how the first head attends to different tokens.\n",
    "\n",
    "\n",
    "In the tokens, you'll see: \n",
    "\n",
    "* [CLS]: This token appears at the beginning of the sequence and is used to capture the overall context of the input\n",
    "* [SEP]: This token appears at the end of the sequence, indicating the end of the input.\n",
    "\n",
    "\n",
    "--------------------------\n",
    "\n",
    "# >>> Your task\n",
    "\n",
    "* Play around with different sentences\n",
    "* Try visualizing different attention heads\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4678e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b9e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606b506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "325aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize self-attention heatmap\n",
    "def visualize_self_attention(sentence, head_to_viz):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "\n",
    "    # Tokenize input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Get model outputs\n",
    "    outputs = model(**inputs)\n",
    "    attention = outputs.attentions[-1]  # Get the attention from the last layer\n",
    "\n",
    "    # Process attention scores\n",
    "    attention = attention.squeeze(0).detach().numpy()  # Remove batch dimension\n",
    "    token_ids = inputs[\"input_ids\"].squeeze(0).numpy()  # Remove batch dimension\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "    # Display tokens for clarity\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "    # Visualize attention heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(attention[head_to_viz], xticklabels=tokens, yticklabels=tokens, cmap='viridis', ax=ax)\n",
    "    plt.title(\"Self-Attention Heatmap\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047281e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "sentence = \"The girl poured water into a tall glass until it overflowed.\"\n",
    "\n",
    "# Visualize self-attention for the example sentence\n",
    "visualize_self_attention(sentence, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd9aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
