{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2cb1dc",
   "metadata": {},
   "source": [
    "# Evaluating how well a language model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ca9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from pprint import pprint\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5427cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b0792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key here\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025f223",
   "metadata": {},
   "source": [
    "## (1) Checking model performance using next token probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738bbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf86a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(id='cmpl-9ZHHS0d7zWKhpcglACeLyBhePNu9m', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=Logprobs(text_offset=[39, 43, 44], token_logprobs=[-0.03429916, -2.5415063, -1.0932422], tokens=[' dog', '\"', ' \\n'], top_logprobs=[{' dog': -0.03429916, ' dogs': -4.873874, ' cat': -5.661428, ' brown': -5.7598166, ' d': -6.34283}, {'\"': -2.5415063, '\\n\\n': -1.8044438, '.\\n\\n': -2.2798486, '”': -2.6530614, '.': -2.7223058}, {' \\n': -1.0932422, ' has': -2.759645, ' is': -2.967853, ' -': -3.0105128, ' in': -3.1142306}]), text=' dog\" \\n')], created=1718194934, model='davinci-002', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=3, prompt_tokens=8, total_tokens=11))\n"
     ]
    }
   ],
   "source": [
    "def get_token_probabilities(prompt, model=\"davinci-002\", max_tokens=3, logprobs=5):\n",
    "    \"\"\"\n",
    "    Get token probabilities for a given prompt using the OpenAI API.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt: str. The input text to generate completions for.\n",
    "    - model: str, optional. The model to use for generating completions.\n",
    "    - max_tokens: int, optional. The maximum number of tokens to generate.\n",
    "    - logprobs: int, optional. The number of log probabilities to return for each token.\n",
    "    \n",
    "    Returns:\n",
    "    - response: dict. The API response containing token probabilities.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            logprobs=logprobs\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "prompt_text = \"The quick brown fox jumps over the lazy\"\n",
    "response = get_token_probabilities(prompt_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbc2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = response.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b48f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  dog\n",
      "   dog: 0.9663\n",
      "   dogs: 0.0076\n",
      "   cat: 0.0035\n",
      "   brown: 0.0032\n",
      "   d: 0.0018\n",
      "\n",
      "Token: \"\n",
      "  \": 0.0787\n",
      "  \n",
      "\n",
      ": 0.1646\n",
      "  .\n",
      "\n",
      ": 0.1023\n",
      "  ”: 0.0704\n",
      "  .: 0.0657\n",
      "\n",
      "Token:  \n",
      "\n",
      "   \n",
      ": 0.3351\n",
      "   has: 0.0633\n",
      "   is: 0.0514\n",
      "   -: 0.0493\n",
      "   in: 0.0444\n",
      "\n"
     ]
    }
   ],
   "source": [
    " if choices:\n",
    "    # Focus on the first choice's 'logprobs' for simplicity\n",
    "    logprobs = choices[0].logprobs\n",
    "    if logprobs:\n",
    "        # Iterate over all tokens and probabilities\n",
    "        for token, token_probs in zip(logprobs.tokens, logprobs.top_logprobs):\n",
    "            # Print each token and its highest probability\n",
    "            print(f\"Token: {token}\")\n",
    "            for prob_token, prob_value in token_probs.items():\n",
    "                print(f\"  {prob_token}: {math.exp(prob_value):.4f}\")\n",
    "            print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b89f",
   "metadata": {},
   "source": [
    "## (2) Calculating ROUGE scores\n",
    "\n",
    "ROUGE-N is a measure of **the overlap of n-grams between the generated text and the reference text.**\n",
    "\n",
    "* ROUGE-1: Overlap of unigrams (individual words).\n",
    "* ROUGE-2: Overlap of bigrams (two-word pairs).\n",
    "* ROUGE-N: Overlap of n-grams (where n can be any number).\n",
    "\n",
    "\"ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\" [source](https://huggingface.co/spaces/evaluate-metric/rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d09671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model=\"davinci-002\", max_tokens=10):\n",
    "    \"\"\"\n",
    "    Generate text using the OpenAI API.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt: str. The input text to generate completions for.\n",
    "    - model: str, optional. The model to use for generating completions.\n",
    "    - max_tokens: int, optional. The maximum number of tokens to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: str. The text generated by the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        generated_text = response.choices[0].text.strip()\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12726231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(generated_text, reference_text):\n",
    "    \"\"\"\n",
    "    Evaluate the ROUGE score between generated and reference texts.\n",
    "    \n",
    "    Parameters:\n",
    "    - generated_text: str. The text generated by the model.\n",
    "    - reference_text: str. The reference text to compare against.\n",
    "    \n",
    "    Returns:\n",
    "    - rouge_scores: dict. The ROUGE scores.\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_text, reference_text, avg=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4196d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "prompt_text = \"The quick brown fox\"\n",
    "reference_text = \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "858dfc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumped over the lazy dog\"\n",
      "\n",
      "The spell check of\n"
     ]
    }
   ],
   "source": [
    "# Generate text using GPT-3\n",
    "generated_text = generate_text(prompt_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c60da588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: jumped over the lazy dog\"\n",
      "\n",
      "The spell check of\n",
      "Reference Text: The quick brown fox jumps over the lazy dog.\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.4444444444444444, 'p': 0.4444444444444444, 'f': 0.44444443944444445}, 'rouge-2': {'r': 0.25, 'p': 0.25, 'f': 0.24999999500000009}, 'rouge-l': {'r': 0.3333333333333333, 'p': 0.3333333333333333, 'f': 0.3333333283333334}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ROUGE score\n",
    "rouge_scores = evaluate_rouge(generated_text, reference_text)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(\"Reference Text:\", reference_text)\n",
    "print(\"ROUGE Scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95ece1",
   "metadata": {},
   "source": [
    "## (3) Calculating BLEU scores\n",
    "\n",
    "Definition: \"BLEU evaluates the degree to which the text generated by the LLM is comparable to human-written material, going beyond simple factual accuracy.\n",
    "\n",
    "It contrasts the produced text with professionally translated human references. Through the analysis of factors such as n-gram overlap (the frequency with which word sequences occur in both the produced text and that reference—BLEU generates a score that accounts for readability, fluency, and grammatical accuracy.\" [source](https://www.labellerr.com/blog/evaluating-large-language-models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a91c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(generated_text, reference_text):\n",
    "    \"\"\"\n",
    "    Evaluate the BLEU score between generated and reference texts.\n",
    "    \n",
    "    Parameters:\n",
    "    - generated_text: str. The text generated by the model.\n",
    "    - reference_text: str. The reference text to compare against.\n",
    "    \n",
    "    Returns:\n",
    "    - bleu_score: float. The BLEU score.\n",
    "    \"\"\"\n",
    "    bleu = sacrebleu.corpus_bleu([generated_text], [[reference_text]])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6186797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: jumped over the lazy dog\"\n",
      "\n",
      "The spell check of\n",
      "Reference Text: The quick brown fox jumps over the lazy dog.\n",
      "BLEU Score: 27.77619034011791\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BLEU score\n",
    "bleu_score = evaluate_bleu(generated_text, reference_text)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(\"Reference Text:\", reference_text)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bc1bf",
   "metadata": {},
   "source": [
    "## (4) Evaluating perplexity \n",
    "\n",
    "Definition: \"[P]erplexity measures the confidence a model has in its (next word) predictions. The concept of perplexity measures how confused the model is in predicting the next word in a sequence.\n",
    "\n",
    "Lower perplexity indicates that the model is more certain about its predictions. In comparison, higher perplexity suggests the model is more uncertain. Perplexity is a crucial metric for evaluating the performance of language models in tasks like machine translation, speech recognition, and text generation.\" [source](https://blog.uptrain.ai/decoding-perplexity-and-its-significance-in-llms/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa7a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, model=\"davinci-002\"):\n",
    "    # Tokenize the input text\n",
    "    tokens = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=text,\n",
    "        max_tokens=0,\n",
    "        logprobs=1,\n",
    "        echo=True\n",
    "    ).choices[0].logprobs.tokens\n",
    "\n",
    "    # Get log probabilities of the tokens\n",
    "    logprobs = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=tokens,\n",
    "        max_tokens=1,\n",
    "        logprobs=1\n",
    "    ).choices[0].logprobs.token_logprobs\n",
    "\n",
    "    # Calculate perplexity\n",
    "    N = len(logprobs)\n",
    "    log_sum = sum(logprobs)\n",
    "    perplexity = math.exp(-log_sum / N)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7edecfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      ", iPhone costs $8 billion to make, and\n",
      "\n",
      "Perplexity of the generated text: 27.9051204315316\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "perplexity = calculate_perplexity(generated_text)\n",
    "print(\"\\nPerplexity of the generated text:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36998308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      ", sports betting online existed solely […]\n",
      "\n",
      "In this V\n",
      "\n",
      "Perplexity of the generated text: 15970.29958070121\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "\n",
    "perplexity = calculate_perplexity(generated_text)\n",
    "print(\"\\nPerplexity of the generated text:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33c94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
